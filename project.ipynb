{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models to Try**\n",
    "- Kernel SVM - https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "- LogisticRegression - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- DecisionTreeClassifier - https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "- RandomForestClassifier - https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "- XGBClassifier - https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html#training-with-scikit-learn-interface\n",
    "- LGBMClassifier - https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html\n",
    "- GBM\n",
    "\n",
    "**Cross Validation**\n",
    "- RandomizedSearchCV - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "- GridSearchCV - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "- StratifiedKFold - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "- 5 - 10 Fold?\n",
    "\n",
    "**Scaling ?**\n",
    "- StandardScaler - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "- MinMaxScaler - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "- RobustScaler - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html\n",
    "\n",
    "**Ways to Address Imbalanced**\n",
    "- NearMiss - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.NearMiss.html\n",
    "- SMOTE - https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n",
    "- SMOTENC - https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.html\n",
    "- SMOTEENN - https://imbalanced-learn.org/stable/references/generated/imblearn.combine.SMOTEENN.html#imblearn.combine.SMOTEENN\n",
    "- SKLearn Parameter class_weight=\"auto\" or \"balanced\" - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "**Metrics**\n",
    "- F1-Score - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "- Precision - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "- Recall - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\n",
    "- AUC (Area Under Curve) - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n",
    "- Confusion Matrix - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "- ROC Curve (Receiver Operating Characteristic) - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Basic Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Model Selection / Cross Validation\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "\n",
    "# Pipelines\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Imbalanced Dataset\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, make_scorer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_curve, auc, roc_auc_score\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Other Techniques\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from scipy.stats import uniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Base dataframe\n",
    "df = pd.read_csv('data/Base.csv')\n",
    "\n",
    "# Separate out the fraud labels\n",
    "y = df['fraud_bool']\n",
    "\n",
    "# Keep features minus fraud label\n",
    "X = df.drop('fraud_bool', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 31 columns):\n",
      " #   Column                            Non-Null Count    Dtype  \n",
      "---  ------                            --------------    -----  \n",
      " 0   income                            1000000 non-null  float64\n",
      " 1   name_email_similarity             1000000 non-null  float64\n",
      " 2   prev_address_months_count         1000000 non-null  int64  \n",
      " 3   current_address_months_count      1000000 non-null  int64  \n",
      " 4   customer_age                      1000000 non-null  int64  \n",
      " 5   days_since_request                1000000 non-null  float64\n",
      " 6   intended_balcon_amount            1000000 non-null  float64\n",
      " 7   payment_type                      1000000 non-null  object \n",
      " 8   zip_count_4w                      1000000 non-null  int64  \n",
      " 9   velocity_6h                       1000000 non-null  float64\n",
      " 10  velocity_24h                      1000000 non-null  float64\n",
      " 11  velocity_4w                       1000000 non-null  float64\n",
      " 12  bank_branch_count_8w              1000000 non-null  int64  \n",
      " 13  date_of_birth_distinct_emails_4w  1000000 non-null  int64  \n",
      " 14  employment_status                 1000000 non-null  object \n",
      " 15  credit_risk_score                 1000000 non-null  int64  \n",
      " 16  email_is_free                     1000000 non-null  int64  \n",
      " 17  housing_status                    1000000 non-null  object \n",
      " 18  phone_home_valid                  1000000 non-null  int64  \n",
      " 19  phone_mobile_valid                1000000 non-null  int64  \n",
      " 20  bank_months_count                 1000000 non-null  int64  \n",
      " 21  has_other_cards                   1000000 non-null  int64  \n",
      " 22  proposed_credit_limit             1000000 non-null  float64\n",
      " 23  foreign_request                   1000000 non-null  int64  \n",
      " 24  source                            1000000 non-null  object \n",
      " 25  session_length_in_minutes         1000000 non-null  float64\n",
      " 26  device_os                         1000000 non-null  object \n",
      " 27  keep_alive_session                1000000 non-null  int64  \n",
      " 28  device_distinct_emails_8w         1000000 non-null  int64  \n",
      " 29  device_fraud_count                1000000 non-null  int64  \n",
      " 30  month                             1000000 non-null  int64  \n",
      "dtypes: float64(9), int64(17), object(5)\n",
      "memory usage: 236.5+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income                                   9\n",
       "name_email_similarity               998861\n",
       "prev_address_months_count              374\n",
       "current_address_months_count           423\n",
       "customer_age                             9\n",
       "days_since_request                  989330\n",
       "intended_balcon_amount              994971\n",
       "payment_type                             5\n",
       "zip_count_4w                          6306\n",
       "velocity_6h                         998687\n",
       "velocity_24h                        998940\n",
       "velocity_4w                         998318\n",
       "bank_branch_count_8w                  2326\n",
       "date_of_birth_distinct_emails_4w        40\n",
       "employment_status                        7\n",
       "credit_risk_score                      551\n",
       "email_is_free                            2\n",
       "housing_status                           7\n",
       "phone_home_valid                         2\n",
       "phone_mobile_valid                       2\n",
       "bank_months_count                       33\n",
       "has_other_cards                          2\n",
       "proposed_credit_limit                   12\n",
       "foreign_request                          2\n",
       "source                                   2\n",
       "session_length_in_minutes           994887\n",
       "device_os                                5\n",
       "keep_alive_session                       2\n",
       "device_distinct_emails_8w                4\n",
       "device_fraud_count                       1\n",
       "month                                    8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See unique number of values in features\n",
    "X.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'device_fraud_count' from X_data as it only has a single unique value in the dataset\n",
    "X = X.drop(labels='device_fraud_count', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['payment_type', 'employment_status', 'housing_status', 'source',\n",
      "       'device_os'],\n",
      "      dtype='object')\n",
      "(1000000, 30)\n",
      "(1000000, 51)\n"
     ]
    }
   ],
   "source": [
    "# One Hot Vector Encoding for Category Features\n",
    "cat_columns = X.select_dtypes('object').columns\n",
    "print(cat_columns)\n",
    "print(X.shape)\n",
    "\n",
    "X = pd.get_dummies(X, columns=cat_columns)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train -  [791176   8824]   |   test -  [197795   2205]\n",
      "train -  [791177   8823]   |   test -  [197794   2206]\n",
      "train -  [791177   8823]   |   test -  [197794   2206]\n",
      "train -  [791177   8823]   |   test -  [197794   2206]\n",
      "train -  [791177   8823]   |   test -  [197794   2206]\n"
     ]
    }
   ],
   "source": [
    "# Stratified 5 K-Fold Train / Test Split\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for train_strat, test_strat in skf.split(X, y):\n",
    "    print('train -  {}   |   test -  {}'.format(np.bincount(y[train_strat]), np.bincount(y[test_strat])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the mean and standard deviation of cross-validated scores\n",
    "def cv_scores(clf, X, y, scoring):\n",
    "    print('Scoring: ', clf, scoring)\n",
    "    scores = cross_val_score(clf, X, y, cv=skf, scoring=scoring)\n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "# Define the models and preprocessing techniques to compare\n",
    "models = {\n",
    "    #'Kernel SVM': SVC(kernel='linear', class_weight='balanced'),\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced'),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'LightGBM': LGBMClassifier()\n",
    "}\n",
    "\n",
    "# Define scaling techniques\n",
    "preprocessing_techniques = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler()\n",
    "}\n",
    "\n",
    "# Define resampling to address imbalance data\n",
    "resampling_techniques = {\n",
    "    'No Resampling': None,\n",
    "    'SMOTE': SMOTE(),\n",
    "    'NearMiss': NearMiss()\n",
    "}\n",
    "\n",
    "# Define metrics to evaluate models\n",
    "scoring = {\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1_score': make_scorer(f1_score),\n",
    "    'roc_auc': make_scorer(roc_auc_score)\n",
    "}\n",
    "\n",
    "# Define a function to run the comparison\n",
    "def compare_models(X, y, models, preprocessing_techniques, resampling_techniques, scoring):\n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        for scaler_name, scaler in preprocessing_techniques.items():\n",
    "            for resampler_name, resampler in resampling_techniques.items():\n",
    "                # Define the pipeline\n",
    "                if resampler:\n",
    "                    pipeline = ImbPipeline([\n",
    "                        ('scaler', scaler),\n",
    "                        ('resampler', resampler),\n",
    "                        ('clf', model)\n",
    "                    ])\n",
    "                else:\n",
    "                    pipeline = make_pipeline(scaler, model)\n",
    "                print('Running: ', pipeline)\n",
    "                # Calculate cross-validated scores\n",
    "                scores = {}\n",
    "                for metric_name, metric in scoring.items():\n",
    "                    mean_score, std_score = cv_scores(pipeline, X, y, metric)\n",
    "                    scores[metric_name] = mean_score\n",
    "\n",
    "                result = {\n",
    "                    'Model': model_name,\n",
    "                    'Scaler': scaler_name,\n",
    "                    'Resampler': resampler_name,\n",
    "                    **scores\n",
    "                }\n",
    "                results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the comparison\n",
    "#comparison_results = compare_models(X, y, models, preprocessing_techniques, resampling_techniques, scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as .pkl and .csv\n",
    "# comparison_results.to_pickle('model_scores.pkl')  # where to save it, usually as a .pkl\n",
    "# comparison_results.to_csv('model_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Resampler</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.045458</td>\n",
       "      <td>0.772132</td>\n",
       "      <td>0.085529</td>\n",
       "      <td>0.792166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.048095</td>\n",
       "      <td>0.750461</td>\n",
       "      <td>0.089876</td>\n",
       "      <td>0.788204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.009823</td>\n",
       "      <td>0.773877</td>\n",
       "      <td>0.019398</td>\n",
       "      <td>0.453230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.045492</td>\n",
       "      <td>0.772585</td>\n",
       "      <td>0.085591</td>\n",
       "      <td>0.792414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.046841</td>\n",
       "      <td>0.756264</td>\n",
       "      <td>0.087684</td>\n",
       "      <td>0.788026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.009751</td>\n",
       "      <td>0.778952</td>\n",
       "      <td>0.019260</td>\n",
       "      <td>0.449271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.045466</td>\n",
       "      <td>0.772676</td>\n",
       "      <td>0.085545</td>\n",
       "      <td>0.792391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.048375</td>\n",
       "      <td>0.749736</td>\n",
       "      <td>0.090473</td>\n",
       "      <td>0.788792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.009971</td>\n",
       "      <td>0.781585</td>\n",
       "      <td>0.019689</td>\n",
       "      <td>0.459528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.053938</td>\n",
       "      <td>0.074799</td>\n",
       "      <td>0.058710</td>\n",
       "      <td>0.529802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.053250</td>\n",
       "      <td>0.175893</td>\n",
       "      <td>0.074485</td>\n",
       "      <td>0.568115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.010392</td>\n",
       "      <td>0.828731</td>\n",
       "      <td>0.020576</td>\n",
       "      <td>0.476598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.053241</td>\n",
       "      <td>0.078244</td>\n",
       "      <td>0.059870</td>\n",
       "      <td>0.528735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.040659</td>\n",
       "      <td>0.167822</td>\n",
       "      <td>0.063395</td>\n",
       "      <td>0.561324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.010340</td>\n",
       "      <td>0.830181</td>\n",
       "      <td>0.020248</td>\n",
       "      <td>0.472101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.053857</td>\n",
       "      <td>0.075524</td>\n",
       "      <td>0.060382</td>\n",
       "      <td>0.528893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.052063</td>\n",
       "      <td>0.164288</td>\n",
       "      <td>0.070025</td>\n",
       "      <td>0.561326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.010443</td>\n",
       "      <td>0.841423</td>\n",
       "      <td>0.020644</td>\n",
       "      <td>0.477894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.104826</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>0.004606</td>\n",
       "      <td>0.501670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.239396</td>\n",
       "      <td>0.071623</td>\n",
       "      <td>0.088172</td>\n",
       "      <td>0.531791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.010502</td>\n",
       "      <td>0.898547</td>\n",
       "      <td>0.020740</td>\n",
       "      <td>0.477627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.281701</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.006960</td>\n",
       "      <td>0.501229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.135083</td>\n",
       "      <td>0.054125</td>\n",
       "      <td>0.076576</td>\n",
       "      <td>0.526051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.904711</td>\n",
       "      <td>0.020769</td>\n",
       "      <td>0.477291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.164848</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.501149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.205929</td>\n",
       "      <td>0.045966</td>\n",
       "      <td>0.070955</td>\n",
       "      <td>0.523151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.010544</td>\n",
       "      <td>0.914141</td>\n",
       "      <td>0.020876</td>\n",
       "      <td>0.480172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.250589</td>\n",
       "      <td>0.138716</td>\n",
       "      <td>0.097936</td>\n",
       "      <td>0.562628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.147455</td>\n",
       "      <td>0.263013</td>\n",
       "      <td>0.114237</td>\n",
       "      <td>0.613782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.010484</td>\n",
       "      <td>0.909064</td>\n",
       "      <td>0.020728</td>\n",
       "      <td>0.476901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.248126</td>\n",
       "      <td>0.135633</td>\n",
       "      <td>0.097101</td>\n",
       "      <td>0.561385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.136220</td>\n",
       "      <td>0.268815</td>\n",
       "      <td>0.115870</td>\n",
       "      <td>0.615509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.010507</td>\n",
       "      <td>0.914504</td>\n",
       "      <td>0.020775</td>\n",
       "      <td>0.477588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.250143</td>\n",
       "      <td>0.139532</td>\n",
       "      <td>0.098767</td>\n",
       "      <td>0.562860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.167231</td>\n",
       "      <td>0.216685</td>\n",
       "      <td>0.132130</td>\n",
       "      <td>0.600802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.010518</td>\n",
       "      <td>0.911874</td>\n",
       "      <td>0.020795</td>\n",
       "      <td>0.478255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.208675</td>\n",
       "      <td>0.147419</td>\n",
       "      <td>0.091424</td>\n",
       "      <td>0.566038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.211657</td>\n",
       "      <td>0.209429</td>\n",
       "      <td>0.119523</td>\n",
       "      <td>0.592728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.010492</td>\n",
       "      <td>0.913507</td>\n",
       "      <td>0.020745</td>\n",
       "      <td>0.477115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.209888</td>\n",
       "      <td>0.149141</td>\n",
       "      <td>0.092794</td>\n",
       "      <td>0.566393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.186116</td>\n",
       "      <td>0.200273</td>\n",
       "      <td>0.111894</td>\n",
       "      <td>0.593391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.010536</td>\n",
       "      <td>0.917767</td>\n",
       "      <td>0.020832</td>\n",
       "      <td>0.478813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.216430</td>\n",
       "      <td>0.142069</td>\n",
       "      <td>0.090138</td>\n",
       "      <td>0.563284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.179937</td>\n",
       "      <td>0.180056</td>\n",
       "      <td>0.127804</td>\n",
       "      <td>0.579745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.915320</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.477412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model          Scaler      Resampler  precision    recall  \\\n",
       "0   Logistic Regression  StandardScaler  No Resampling   0.045458  0.772132   \n",
       "1   Logistic Regression  StandardScaler          SMOTE   0.048095  0.750461   \n",
       "2   Logistic Regression  StandardScaler       NearMiss   0.009823  0.773877   \n",
       "3   Logistic Regression    MinMaxScaler  No Resampling   0.045492  0.772585   \n",
       "4   Logistic Regression    MinMaxScaler          SMOTE   0.046841  0.756264   \n",
       "5   Logistic Regression    MinMaxScaler       NearMiss   0.009751  0.778952   \n",
       "6   Logistic Regression    RobustScaler  No Resampling   0.045466  0.772676   \n",
       "7   Logistic Regression    RobustScaler          SMOTE   0.048375  0.749736   \n",
       "8   Logistic Regression    RobustScaler       NearMiss   0.009971  0.781585   \n",
       "9         Decision Tree  StandardScaler  No Resampling   0.053938  0.074799   \n",
       "10        Decision Tree  StandardScaler          SMOTE   0.053250  0.175893   \n",
       "11        Decision Tree  StandardScaler       NearMiss   0.010392  0.828731   \n",
       "12        Decision Tree    MinMaxScaler  No Resampling   0.053241  0.078244   \n",
       "13        Decision Tree    MinMaxScaler          SMOTE   0.040659  0.167822   \n",
       "14        Decision Tree    MinMaxScaler       NearMiss   0.010340  0.830181   \n",
       "15        Decision Tree    RobustScaler  No Resampling   0.053857  0.075524   \n",
       "16        Decision Tree    RobustScaler          SMOTE   0.052063  0.164288   \n",
       "17        Decision Tree    RobustScaler       NearMiss   0.010443  0.841423   \n",
       "18        Random Forest  StandardScaler  No Resampling   0.104826  0.002357   \n",
       "19        Random Forest  StandardScaler          SMOTE   0.239396  0.071623   \n",
       "20        Random Forest  StandardScaler       NearMiss   0.010502  0.898547   \n",
       "21        Random Forest    MinMaxScaler  No Resampling   0.281701  0.002629   \n",
       "22        Random Forest    MinMaxScaler          SMOTE   0.135083  0.054125   \n",
       "23        Random Forest    MinMaxScaler       NearMiss   0.010488  0.904711   \n",
       "24        Random Forest    RobustScaler  No Resampling   0.164848  0.002448   \n",
       "25        Random Forest    RobustScaler          SMOTE   0.205929  0.045966   \n",
       "26        Random Forest    RobustScaler       NearMiss   0.010544  0.914141   \n",
       "27              XGBoost  StandardScaler  No Resampling   0.250589  0.138716   \n",
       "28              XGBoost  StandardScaler          SMOTE   0.147455  0.263013   \n",
       "29              XGBoost  StandardScaler       NearMiss   0.010484  0.909064   \n",
       "30              XGBoost    MinMaxScaler  No Resampling   0.248126  0.135633   \n",
       "31              XGBoost    MinMaxScaler          SMOTE   0.136220  0.268815   \n",
       "32              XGBoost    MinMaxScaler       NearMiss   0.010507  0.914504   \n",
       "33              XGBoost    RobustScaler  No Resampling   0.250143  0.139532   \n",
       "34              XGBoost    RobustScaler          SMOTE   0.167231  0.216685   \n",
       "35              XGBoost    RobustScaler       NearMiss   0.010518  0.911874   \n",
       "36             LightGBM  StandardScaler  No Resampling   0.208675  0.147419   \n",
       "37             LightGBM  StandardScaler          SMOTE   0.211657  0.209429   \n",
       "38             LightGBM  StandardScaler       NearMiss   0.010492  0.913507   \n",
       "39             LightGBM    MinMaxScaler  No Resampling   0.209888  0.149141   \n",
       "40             LightGBM    MinMaxScaler          SMOTE   0.186116  0.200273   \n",
       "41             LightGBM    MinMaxScaler       NearMiss   0.010536  0.917767   \n",
       "42             LightGBM    RobustScaler  No Resampling   0.216430  0.142069   \n",
       "43             LightGBM    RobustScaler          SMOTE   0.179937  0.180056   \n",
       "44             LightGBM    RobustScaler       NearMiss   0.010500  0.915320   \n",
       "\n",
       "    f1_score   roc_auc  \n",
       "0   0.085529  0.792166  \n",
       "1   0.089876  0.788204  \n",
       "2   0.019398  0.453230  \n",
       "3   0.085591  0.792414  \n",
       "4   0.087684  0.788026  \n",
       "5   0.019260  0.449271  \n",
       "6   0.085545  0.792391  \n",
       "7   0.090473  0.788792  \n",
       "8   0.019689  0.459528  \n",
       "9   0.058710  0.529802  \n",
       "10  0.074485  0.568115  \n",
       "11  0.020576  0.476598  \n",
       "12  0.059870  0.528735  \n",
       "13  0.063395  0.561324  \n",
       "14  0.020248  0.472101  \n",
       "15  0.060382  0.528893  \n",
       "16  0.070025  0.561326  \n",
       "17  0.020644  0.477894  \n",
       "18  0.004606  0.501670  \n",
       "19  0.088172  0.531791  \n",
       "20  0.020740  0.477627  \n",
       "21  0.006960  0.501229  \n",
       "22  0.076576  0.526051  \n",
       "23  0.020769  0.477291  \n",
       "24  0.004595  0.501149  \n",
       "25  0.070955  0.523151  \n",
       "26  0.020876  0.480172  \n",
       "27  0.097936  0.562628  \n",
       "28  0.114237  0.613782  \n",
       "29  0.020728  0.476901  \n",
       "30  0.097101  0.561385  \n",
       "31  0.115870  0.615509  \n",
       "32  0.020775  0.477588  \n",
       "33  0.098767  0.562860  \n",
       "34  0.132130  0.600802  \n",
       "35  0.020795  0.478255  \n",
       "36  0.091424  0.566038  \n",
       "37  0.119523  0.592728  \n",
       "38  0.020745  0.477115  \n",
       "39  0.092794  0.566393  \n",
       "40  0.111894  0.593391  \n",
       "41  0.020832  0.478813  \n",
       "42  0.090138  0.563284  \n",
       "43  0.127804  0.579745  \n",
       "44  0.020761  0.477412  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_results = df = pd.read_pickle('model_scores.pkl')\n",
    "comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running:  Pipeline(steps=[('nonetype', None),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000))])\n",
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000))]) make_scorer(precision_score, response_method='predict')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000))]) make_scorer(recall_score, response_method='predict')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000))]) make_scorer(f1_score, response_method='predict')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000))]) make_scorer(roc_auc_score, response_method='predict')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000))])\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000))]) make_scorer(precision_score, response_method='predict')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000))]) make_scorer(recall_score, response_method='predict')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000))]) make_scorer(f1_score, response_method='predict')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000))]) make_scorer(roc_auc_score, response_method='predict')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000))])\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000))]) make_scorer(precision_score, response_method='predict')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000))]) make_scorer(recall_score, response_method='predict')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000))]) make_scorer(f1_score, response_method='predict')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000))]) make_scorer(roc_auc_score, response_method='predict')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running:  Pipeline(steps=[('nonetype', None),\n",
      "                ('decisiontreeclassifier',\n",
      "                 DecisionTreeClassifier(class_weight='balanced'))])\n",
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('decisiontreeclassifier',\n",
      "                 DecisionTreeClassifier(class_weight='balanced'))]) make_scorer(precision_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('decisiontreeclassifier',\n",
      "                 DecisionTreeClassifier(class_weight='balanced'))]) make_scorer(recall_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('decisiontreeclassifier',\n",
      "                 DecisionTreeClassifier(class_weight='balanced'))]) make_scorer(f1_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('decisiontreeclassifier',\n",
      "                 DecisionTreeClassifier(class_weight='balanced'))]) make_scorer(roc_auc_score, response_method='predict')\n",
      "Running:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf', DecisionTreeClassifier(class_weight='balanced'))])\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf', DecisionTreeClassifier(class_weight='balanced'))]) make_scorer(precision_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf', DecisionTreeClassifier(class_weight='balanced'))]) make_scorer(recall_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf', DecisionTreeClassifier(class_weight='balanced'))]) make_scorer(f1_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf', DecisionTreeClassifier(class_weight='balanced'))]) make_scorer(roc_auc_score, response_method='predict')\n",
      "Running:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf', DecisionTreeClassifier(class_weight='balanced'))])\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf', DecisionTreeClassifier(class_weight='balanced'))]) make_scorer(precision_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf', DecisionTreeClassifier(class_weight='balanced'))]) make_scorer(recall_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf', DecisionTreeClassifier(class_weight='balanced'))]) make_scorer(f1_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf', DecisionTreeClassifier(class_weight='balanced'))]) make_scorer(roc_auc_score, response_method='predict')\n",
      "Running:  Pipeline(steps=[('nonetype', None),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(class_weight='balanced'))])\n",
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(class_weight='balanced'))]) make_scorer(precision_score, response_method='predict')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/.pyenv/versions/3.12.1/envs/orie5741/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(class_weight='balanced'))]) make_scorer(recall_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(class_weight='balanced'))]) make_scorer(f1_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(class_weight='balanced'))]) make_scorer(roc_auc_score, response_method='predict')\n",
      "Running:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf', RandomForestClassifier(class_weight='balanced'))])\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf', RandomForestClassifier(class_weight='balanced'))]) make_scorer(precision_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf', RandomForestClassifier(class_weight='balanced'))]) make_scorer(recall_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf', RandomForestClassifier(class_weight='balanced'))]) make_scorer(f1_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf', RandomForestClassifier(class_weight='balanced'))]) make_scorer(roc_auc_score, response_method='predict')\n",
      "Running:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf', RandomForestClassifier(class_weight='balanced'))])\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf', RandomForestClassifier(class_weight='balanced'))]) make_scorer(precision_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf', RandomForestClassifier(class_weight='balanced'))]) make_scorer(recall_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf', RandomForestClassifier(class_weight='balanced'))]) make_scorer(f1_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf', RandomForestClassifier(class_weight='balanced'))]) make_scorer(roc_auc_score, response_method='predict')\n",
      "Running:  Pipeline(steps=[('nonetype', None),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))])\n",
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))]) make_scorer(precision_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))]) make_scorer(recall_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))]) make_scorer(f1_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('nonetype', None),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))]) make_scorer(roc_auc_score, response_method='predict')\n",
      "Running:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))])\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))]) make_scorer(precision_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))]) make_scorer(recall_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))]) make_scorer(f1_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))]) make_scorer(roc_auc_score, response_method='predict')\n",
      "Running:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))])\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))]) make_scorer(precision_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))]) make_scorer(recall_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))]) make_scorer(f1_score, response_method='predict')\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))]) make_scorer(roc_auc_score, response_method='predict')\n",
      "Running:  Pipeline(steps=[('nonetype', None), ('lgbmclassifier', LGBMClassifier())])\n",
      "Scoring:  Pipeline(steps=[('nonetype', None), ('lgbmclassifier', LGBMClassifier())]) make_scorer(precision_score, response_method='predict')\n",
      "[LightGBM] [Info] Number of positive: 8824, number of negative: 791176\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3239\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496045\n",
      "[LightGBM] [Info] Start training from score -4.496045\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3238\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3240\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3239\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3239\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "Scoring:  Pipeline(steps=[('nonetype', None), ('lgbmclassifier', LGBMClassifier())]) make_scorer(recall_score, response_method='predict')\n",
      "[LightGBM] [Info] Number of positive: 8824, number of negative: 791176\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3239\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496045\n",
      "[LightGBM] [Info] Start training from score -4.496045\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3238\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3240\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3239\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3239\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "Scoring:  Pipeline(steps=[('nonetype', None), ('lgbmclassifier', LGBMClassifier())]) make_scorer(f1_score, response_method='predict')\n",
      "[LightGBM] [Info] Number of positive: 8824, number of negative: 791176\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3239\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496045\n",
      "[LightGBM] [Info] Start training from score -4.496045\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3238\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3240\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3239\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3239\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "Scoring:  Pipeline(steps=[('nonetype', None), ('lgbmclassifier', LGBMClassifier())]) make_scorer(roc_auc_score, response_method='predict')\n",
      "[LightGBM] [Info] Number of positive: 8824, number of negative: 791176\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3239\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496045\n",
      "[LightGBM] [Info] Start training from score -4.496045\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3238\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3240\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018654 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3239\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024579 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3239\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "Running:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf', LGBMClassifier())])\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf', LGBMClassifier())]) make_scorer(precision_score, response_method='predict')\n",
      "[LightGBM] [Info] Number of positive: 791176, number of negative: 791176\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3792\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582352, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3793\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3793\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079606 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3796\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082491 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3794\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf', LGBMClassifier())]) make_scorer(recall_score, response_method='predict')\n",
      "[LightGBM] [Info] Number of positive: 791176, number of negative: 791176\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3794\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582352, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3793\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3792\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3797\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3794\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf', LGBMClassifier())]) make_scorer(f1_score, response_method='predict')\n",
      "[LightGBM] [Info] Number of positive: 791176, number of negative: 791176\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3792\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582352, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3794\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3794\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076938 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3794\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3794\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', SMOTE()),\n",
      "                ('clf', LGBMClassifier())]) make_scorer(roc_auc_score, response_method='predict')\n",
      "[LightGBM] [Info] Number of positive: 791176, number of negative: 791176\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3793\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582352, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3794\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3793\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3795\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 791177, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071559 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3795\n",
      "[LightGBM] [Info] Number of data points in the train set: 1582354, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Running:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf', LGBMClassifier())])\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf', LGBMClassifier())]) make_scorer(precision_score, response_method='predict')\n",
      "[LightGBM] [Info] Number of positive: 8824, number of negative: 8824\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001755 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3112\n",
      "[LightGBM] [Info] Number of data points in the train set: 17648, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3113\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3121\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3127\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3109\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf', LGBMClassifier())]) make_scorer(recall_score, response_method='predict')\n",
      "[LightGBM] [Info] Number of positive: 8824, number of negative: 8824\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3112\n",
      "[LightGBM] [Info] Number of data points in the train set: 17648, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3113\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3121\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3127\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3109\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf', LGBMClassifier())]) make_scorer(f1_score, response_method='predict')\n",
      "[LightGBM] [Info] Number of positive: 8824, number of negative: 8824\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3112\n",
      "[LightGBM] [Info] Number of data points in the train set: 17648, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3113\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3121\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3127\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3109\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Scoring:  Pipeline(steps=[('scaler', None), ('resampler', NearMiss()),\n",
      "                ('clf', LGBMClassifier())]) make_scorer(roc_auc_score, response_method='predict')\n",
      "[LightGBM] [Info] Number of positive: 8824, number of negative: 8824\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3112\n",
      "[LightGBM] [Info] Number of data points in the train set: 17648, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3113\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3121\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3127\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 8823\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3109\n",
      "[LightGBM] [Info] Number of data points in the train set: 17646, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    }
   ],
   "source": [
    "preprocessing_techniques = {'No Scaling': None}\n",
    "comparison_results_no_scaling = compare_models(X, y, models, preprocessing_techniques, resampling_techniques, scoring)\n",
    "comparison_results_no_scaling.to_pickle('model_scores_no_scaling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Resampler</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.025876</td>\n",
       "      <td>0.708944</td>\n",
       "      <td>0.049895</td>\n",
       "      <td>0.703894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.045220</td>\n",
       "      <td>0.645121</td>\n",
       "      <td>0.052743</td>\n",
       "      <td>0.677191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>0.814504</td>\n",
       "      <td>0.026709</td>\n",
       "      <td>0.554372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.055276</td>\n",
       "      <td>0.076794</td>\n",
       "      <td>0.060211</td>\n",
       "      <td>0.529015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>0.144978</td>\n",
       "      <td>0.079394</td>\n",
       "      <td>0.556867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.012036</td>\n",
       "      <td>0.925923</td>\n",
       "      <td>0.023826</td>\n",
       "      <td>0.539580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.193203</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.005863</td>\n",
       "      <td>0.501583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.211756</td>\n",
       "      <td>0.051770</td>\n",
       "      <td>0.081782</td>\n",
       "      <td>0.524336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.012018</td>\n",
       "      <td>0.983952</td>\n",
       "      <td>0.023779</td>\n",
       "      <td>0.538942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.248547</td>\n",
       "      <td>0.139169</td>\n",
       "      <td>0.097999</td>\n",
       "      <td>0.562748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.171156</td>\n",
       "      <td>0.229470</td>\n",
       "      <td>0.145540</td>\n",
       "      <td>0.594990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.012080</td>\n",
       "      <td>0.980869</td>\n",
       "      <td>0.023864</td>\n",
       "      <td>0.541123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>No Resampling</td>\n",
       "      <td>0.203941</td>\n",
       "      <td>0.145152</td>\n",
       "      <td>0.090614</td>\n",
       "      <td>0.564873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.199856</td>\n",
       "      <td>0.156759</td>\n",
       "      <td>0.145149</td>\n",
       "      <td>0.576699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>NearMiss</td>\n",
       "      <td>0.012193</td>\n",
       "      <td>0.983952</td>\n",
       "      <td>0.024085</td>\n",
       "      <td>0.544731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model      Scaler      Resampler  precision    recall  \\\n",
       "0   Logistic Regression  No Scaling  No Resampling   0.025876  0.708944   \n",
       "1   Logistic Regression  No Scaling          SMOTE   0.045220  0.645121   \n",
       "2   Logistic Regression  No Scaling       NearMiss   0.013636  0.814504   \n",
       "3         Decision Tree  No Scaling  No Resampling   0.055276  0.076794   \n",
       "4         Decision Tree  No Scaling          SMOTE   0.062337  0.144978   \n",
       "5         Decision Tree  No Scaling       NearMiss   0.012036  0.925923   \n",
       "6         Random Forest  No Scaling  No Resampling   0.193203  0.002448   \n",
       "7         Random Forest  No Scaling          SMOTE   0.211756  0.051770   \n",
       "8         Random Forest  No Scaling       NearMiss   0.012018  0.983952   \n",
       "9               XGBoost  No Scaling  No Resampling   0.248547  0.139169   \n",
       "10              XGBoost  No Scaling          SMOTE   0.171156  0.229470   \n",
       "11              XGBoost  No Scaling       NearMiss   0.012080  0.980869   \n",
       "12             LightGBM  No Scaling  No Resampling   0.203941  0.145152   \n",
       "13             LightGBM  No Scaling          SMOTE   0.199856  0.156759   \n",
       "14             LightGBM  No Scaling       NearMiss   0.012193  0.983952   \n",
       "\n",
       "    f1_score   roc_auc  \n",
       "0   0.049895  0.703894  \n",
       "1   0.052743  0.677191  \n",
       "2   0.026709  0.554372  \n",
       "3   0.060211  0.529015  \n",
       "4   0.079394  0.556867  \n",
       "5   0.023826  0.539580  \n",
       "6   0.005863  0.501583  \n",
       "7   0.081782  0.524336  \n",
       "8   0.023779  0.538942  \n",
       "9   0.097999  0.562748  \n",
       "10  0.145540  0.594990  \n",
       "11  0.023864  0.541123  \n",
       "12  0.090614  0.564873  \n",
       "13  0.145149  0.576699  \n",
       "14  0.024085  0.544731  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_results_no_scaling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orie5741",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
